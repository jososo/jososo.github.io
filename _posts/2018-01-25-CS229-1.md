---
layout: post
title: CS229-1
date:   2018-01-25 21:29:08 +0800
categories: 机器学习
description: CS229
keywords: 机器学习
---

## 线性回归（Linear Regression）、梯度下降（Gradient Descent）、正规方程组（Normal Equation）
    m：训练样本的数量(表的行数)
    
    x：输入变量（也可以称为特征 feature），可能有多个x，一组特征就是特征向量
    
    y：输出变量（目标变量）
    (x，y)：一个训练样本，（xi，yi）第i个训练样本（i非次方，只是标识）
    
    h（x）：训练好的算法（函数），称为假设，它的任务就是接收输入并输出估计值
    这里用到的是$h_θ(x)=θ_0+θ_1x_1+θ_2x_2$
    
    θ：参数（实数）（是个向量值，函数中可能有1-n个θ）

J(θ)：成本函数，用来衡量对于每个不同的 θ 值，h(x(i)) 与对应的 y(i) 的距离
这里是：$$J(θ)=1/2\sum_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})^2$$

## 梯度下降
用于最小二乘回归问题


我们希望选择一个能让J(θ)最小的θ值。怎么做呢，咱们先用一个搜索的算法，从某一个对 θ 的“初始猜测值”（假设为0向量），然后对 θ 值不断进行调整，来让 J(θ) 逐渐变小，最好是直到我们能够达到一个使 J(θ) 最小的 θ

（这是只有两个输入变量的情况下,因为有多个变量所用用梯度表示）
图形和坐标轴，水平方向的坐标轴（x，y）通常表示theta0和theta1，而最小值j就用图像的高度z进行表示
整个图形表示的是最小值j关于theta的函数，x和y决定z
梯度下降图像理解从开始点环视360度找到下降最快的方向(注意，梯度的方向是变化最快的方向，但不是下降最快的方向，负方向才是下降最快的)走一步**（其实就是计算梯度，计算函数的偏导数,其实梯度的正方向是变化最快，但那是向上的，类似一元导数中导数大于0的情况，原函数是递增的，如果我们直接使用找到的是函数的最大值，所以我们要用它的负方向，就是向下的（递减的），才能找到最小值j（θ））**，到了一个新的点（新的xy），再重复一次环视找新点，直到到达函数（图）中局部最小值（类似贪心算法）
换不同起点可能得到不同的局部最小值
梯度下降的结果有时候依赖于初始值（开始点）

---
（相关数学）
梯度: 　在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。比如函数f(x,y), 分别对x,y求偏导数，求得的梯度向量就是(∂f/∂x, ∂f/∂y)T,简称grad f(x,y)或者▽f(x,y)。，其实就是三维空间的导数，本质是个向量。

 梯度的方向就是函数增加最快的方向（反方向就是函数变化最慢），梯度的大小就是沿这个方向的斜率。

举例子来讲会比较简单,如果现在的标量场用一座山来表示,标量值越大的地方越高,反之则越低.经过梯度这个运操作数的运算以后,会在这座山的每一个点上都算出一个向量,这个向量会指向每个点最陡的那个方向,而向量的大小则代表了这个最陡的方向到底有多陡.

---
**批梯度下降（batch gradient descent）**
批是指每次梯度下降都要遍历整个训练集（当训练集很大的时候要遍历整个训练集，效率低）

一个训练集
旧最小值j减去学习速度乘上最小值算式的导数（减就是向最陡方向）

学习速度alpha:向最陡方向前进的步数（大小），如果设置得小，朝最陡方向的步子就小，算法收敛慢，设置过大，可能会越过最小值

多个训练集
更新公式：$$θj=θj−α \sum_{i=1}^{m}(h_θ(x^{(i)})−y^{(i)})  * x_j^{(i)}$$

=：是计算机赋值的意思，新值等于旧值减去学习速度乘上最小值算式的导数
θj：成本函数（求最小值函数）下使用的θ
α:学习速度
$h_θ(x^{(i)})$:假设h(x)计算出来的估计值，即θ0*x0+θ1*x1+...+θn*xn(x0=1)
$x_j^{(i)}$：第i个输入到h（x）的训练样本，所有θ中θ值等于θj（这可能是个向量）时对应的那些x（因为是向量组，所有有很多个x，所以，乘法那里可能是个实数乘向量）

通常图是碗状，只有一个全局最小值（这是这类二元的线性回归的一个性质，一般只有一个局部最小值就是全局最小值）

梯度下降的一个性质，越接近最小值，步子越小（梯度变得越来越小了），最终直到收敛，当达到局部最小值，梯度会变为0（终止条件）

得到最小值，就是得到最小二乘拟合

检测梯度下降是否收敛：
（1）检测两次迭代，看他们最后出来的θ是否有很大变化
（2）检验最求出的最小值J是否发生比较大的变化

**随机梯度下降（Stochastic Gradient Descent）**
随机梯度下降法，其实和批梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据来求出被减的梯度，而是仅仅选取一个样本i来求梯度。(每次更新只使用一个样本)

更新公式：$$θj=θj−α (h_θ(x^{(i)})−y^{(i)})  * x_j^{(i)}$$

通常情况下，随机梯度下降法查找到足够接近最低值的 θ 的速度要比批量梯度下降法更快一些。(也要注意，也有可能会一直无法收敛（converge）到最小值，这时候 θ 会一直在 J(θ) 最小值附近震荡；不过通常情况下在最小值附近的这些值大多数其实也足够逼近了，足以满足咱们的精度要求，所以也可以用。

ps：不使梯度为0的原因可以直接查看梯度下降和梯度上升的文章


## 正规方程组（Normal Equation）
几行公式就能推导出解析表达式
trace 求迹运算，简写为tr(迹的前提是矩阵的行数等于列数)


（1）$trAB=trBA$
证明：
$$（AB）_{ii}=\sum_{k=1}^{n}a_{ik}b_{ki}$$
（矩阵相乘就是前面的行乘后面的列，前面每一个改变的是列号，后面改变的是行号）
$$（BA）_{ii}=\sum_{k=1}^{n}b_{ik}a_{ki}$$
由于i和k都是1到n(矩阵中的每一位都参与了运算，连加中每一种i，k组合都有，一定有对应的),所以左边的aikbki总能在右边找到对应的bikaki与之相等（左边的i=3，k=4，在右边能找到一个对应的i=4，k=3的ab与之相等）
左边的i=3，k=4时有a34b43
右边的i=4，k=3时有b43a34（相等）
$$trAB=\sum_{k=0,i=0}^{n}a_{ik}b_{ki}=\sum_{k=0,i=0}^{n}b_{ik}a_{ki}=trBA$$
同理：trABC=trCAB=trBCA(末尾矩阵循环前移)

（2）$trA=trA^{T}$

（3）$tr(A+B)=trA+trB$


（4）$traA=atrA$

（5）$\nabla_{A}trAB=B^{T}$

原理$trAB=a_{ik}b_{ki}+a_{ik+1}b_{k+1i}+a_{ik+2}b_{k+2i}+...+a_{ik+n}b_{k+ni}$

矩阵求导的前提是有一个函数 f:Rm×n（矩阵）→R(实数)，从m*n大小的矩阵映射到实数域 ，对应这里的就是 f(A) = trAB（输入是矩阵AB，输出是实数trAB）

实际上 f 映射的是从 Rm×n 到实数域 R。这样接下来就可以使用矩阵导数来找到 ∇Af(A) ，这个导函数本身也是一个 m*n 的矩阵

根据矩阵求导的定义，在那个矩阵的aik位置对函数f(A)=trAB求导aik，求导aik只有对应的bki有意义，所以那个位置导出来就是bki，同理ai(k+1)位置导出来的是b(k+1)i，一直这样循环最后整个矩阵就是B的转置矩阵

（6）A为实数 trA（只有一个实数的矩阵）=A

（7）$\nabla_{A}trABA^{T}C=CAB+C^{T}AB^{T}$

## 梯度下降与正规方程组比较

There is no need to do feature scaling with the normal equation.

The following is a comparison of gradient descent and the normal equation:
|Gradient Descent |	Normal Equation |
|:   -----    :| :----:  |
|Need to choose alpha|	No need to choose alpha |
|Needs many iterations |	No need to iterate  |
|O ($kn^2$)	|O ($n^3$), need to calculate inverse of $X^TX$|
|Works well when n is large	|Slow if n is very large|

With the normal equation, computing the inversion has complexity O($n^3$). So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.（超过10000个特征使用梯度下降更好）


http://blog.csdn.net/chan15/article/details/49948849
http://blog.csdn.net/lanchunhui/article/details/53326519





